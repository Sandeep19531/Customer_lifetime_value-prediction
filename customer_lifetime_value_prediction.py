# -*- coding: utf-8 -*-
"""Customer_Lifetime_Value_prediction

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TkIUAl7-mwTiULH-P1ohuZAM3eUfaSNc

# Submitted By: Sandeep
"""

import pandas as pd

! ls

df= pd.read_excel('Online_Retail.xlsx')

df.head()

df.dtypes

df.InvoiceNo.unique

len(df)

df.drop_duplicates(keep='first',inplace=True)
len(df)

df.isnull().sum()

df = df[df['Quantity']>0]
len(df)

df.InvoiceNo = df.InvoiceNo.astype(str)
df.dtypes

#number of canceled transactions
count =0
for i in df.InvoiceNo:
  if i[0] == 'C' or str(i)[0] == 'c':
    count += 1
print(count)

df = df[~df.InvoiceNo.str.contains("C")]
# the number is 0 because we have already remove  quantity with 0 value.
len(df)

df[df.UnitPrice < 0]

df = df.dropna()
len(df)

df.CustomerID = df.CustomerID.astype(str)

df.describe()

df.Country.unique()

df.InvoiceDate.describe()

df1 = df[df['InvoiceDate']>= "2010-12-09"] # one year data
len(df1)
tx_data = df

tx_uk = df[df['Country']=='United Kingdom'].reset_index(drop=True)

tx_uk['InvoiceYearMonth'] = tx_uk['InvoiceDate'].map(lambda date: 100*date.year + date.month)

tx_uk.head()

"""*Segmentation technique*
**RFM**

Recency
"""

tx_user = pd.DataFrame(tx_data['CustomerID'].unique())
tx_user.columns = ['CustomerID']

tx_max_purchase = tx_uk.groupby('CustomerID').InvoiceDate.max().reset_index()
tx_max_purchase.columns = ['CustomerID','MaxPurchaseDate']

tx_max_purchase['Recency'] = (tx_max_purchase['MaxPurchaseDate'].max() - tx_max_purchase['MaxPurchaseDate']).dt.days
tx_max_purchase.head()

tx_user = pd.merge(tx_user, tx_max_purchase[['CustomerID','Recency']], on='CustomerID')
tx_user.head()

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

sse={} # error
tx_recency = tx_user[['Recency']]
for k in range(1, 10):
    kmeans = KMeans(n_clusters=k, max_iter=1000).fit(tx_recency)
    tx_recency["clusters"] = kmeans.labels_  #cluster names corresponding to recency values 
    sse[k] = kmeans.inertia_ #sse corresponding to clusters
plt.figure()
plt.plot(list(sse.keys()), list(sse.values()))
plt.xlabel("Number of cluster")
plt.show()

#going with 3 clusters
kmeans = KMeans(n_clusters=3)
tx_user['RecencyCluster'] = kmeans.fit_predict(tx_user[['Recency']])

tx_user.groupby('RecencyCluster')['Recency'].describe()

def order_cluster(cluster_field_name, target_field_name,df,ascending):
    new_cluster_field_name = 'new_' + cluster_field_name
    df_new = df.groupby(cluster_field_name)[target_field_name].mean().reset_index()
    df_new = df_new.sort_values(by=target_field_name,ascending=ascending).reset_index(drop=True)
    df_new['index'] = df_new.index
    df_final = pd.merge(df,df_new[[cluster_field_name,'index']], on=cluster_field_name)
    df_final = df_final.drop([cluster_field_name],axis=1)
    df_final = df_final.rename(columns={"index":cluster_field_name})
    return df_final

tx_user = order_cluster('RecencyCluster', 'Recency',tx_user,False)

tx_user.groupby('RecencyCluster')['Recency'].describe()

"""**Frequency**"""

tx_frequency = tx_uk.groupby('CustomerID').InvoiceDate.count().reset_index()

tx_frequency.columns = ['CustomerID','Frequency']

tx_user = pd.merge(tx_user, tx_frequency, on='CustomerID')
tx_user.head()

sse={} # error
tx_recency = tx_user[['Frequency']]
for k in range(1, 10):
    kmeans = KMeans(n_clusters=k, max_iter=1000).fit(tx_recency)
    tx_recency["clusters"] = kmeans.labels_  #cluster names corresponding to recency values 
    sse[k] = kmeans.inertia_ #sse corresponding to clusters
plt.figure()
plt.plot(list(sse.keys()), list(sse.values()))
plt.xlabel("Number of cluster")
plt.show()

kmeans=KMeans(n_clusters=4)
tx_user['FrequencyCluster']=kmeans.fit_predict(tx_user[['Frequency']])

#order the frequency cluster
tx_user = order_cluster('FrequencyCluster', 'Frequency', tx_user, True )
tx_user.groupby('FrequencyCluster')['Frequency'].describe()

tx_uk['Revenue'] = tx_uk['UnitPrice'] * tx_uk['Quantity']
tx_revenue = tx_uk.groupby('CustomerID').Revenue.sum().reset_index()

tx_user = pd.merge(tx_user, tx_revenue, on='CustomerID')

tx_revenue.head()

tx_user.head()

sse={} # error
tx_recency = tx_user[['Revenue']]
for k in range(1, 10):
    kmeans = KMeans(n_clusters=k, max_iter=1000).fit(tx_recency)
    tx_recency["clusters"] = kmeans.labels_  #cluster names corresponding to recency values 
    sse[k] = kmeans.inertia_ #sse corresponding to clusters
plt.figure()
plt.plot(list(sse.keys()), list(sse.values()))
plt.xlabel("Number of cluster")
plt.show()

kmeans = KMeans(n_clusters=3)
tx_user['RevenueCluster'] = kmeans.fit_predict(tx_user[['Revenue']])

#order the cluster numbers
tx_user = order_cluster('RevenueCluster', 'Revenue',tx_user,True)

#show details of the dataframe
tx_user.groupby('RevenueCluster')['Revenue'].describe()

tx_user['OverallScore'] = tx_user['RecencyCluster'] + tx_user['FrequencyCluster'] + tx_user['RevenueCluster']
tx_user.groupby('OverallScore')['Recency','Frequency','Revenue'].mean()

tx_user['Segment'] = 'Bronze'
tx_user.loc[tx_user['OverallScore']>2,'Segment'] = 'Silver' 
tx_user.loc[tx_user['OverallScore']>4,'Segment'] = 'Gold'

tx_user.head()

from datetime import date, datetime,timedelta
tx_6m1 = tx_uk[(tx_uk.InvoiceDate < datetime(2011,6,1)) & (tx_uk.InvoiceDate >= datetime(2011,1,1))].reset_index(drop=True) #6 months time
tx_6m2 = tx_uk[(tx_uk.InvoiceDate >= datetime(2011,6,1)) & (tx_uk.InvoiceDate < datetime(2011,12,1))].reset_index(drop=True)

tx_6m2['Revenue'] = tx_6m2['UnitPrice'] * tx_6m2['Quantity']
tx_user_6m = tx_6m2.groupby('CustomerID')['Revenue'].sum().reset_index()
tx_user_6m.columns = ['CustomerID','m6_Revenue']

tx_user_6m.head()

tx_merge = pd.merge(tx_user, tx_user_6m, on='CustomerID', how='left')

tx_merge = tx_merge[tx_merge['m6_Revenue']<tx_merge['m6_Revenue'].quantile(0.99)]

kmeans = KMeans(n_clusters=3)
tx_merge['LTVCluster'] = kmeans.fit_predict(tx_merge[['m6_Revenue']])

tx_merge.head()

tx_merge = order_cluster('LTVCluster', 'm6_Revenue',tx_merge,True)

#creatinga new cluster dataframe
tx_cluster = tx_merge.copy()

#see details of the clusters
tx_cluster.groupby('LTVCluster')['m6_Revenue'].describe()

tx_cluster.head()

tx_class = pd.get_dummies(tx_cluster)

corr_matrix = tx_class.corr()
corr_matrix['LTVCluster'].sort_values(ascending=False)

X = tx_class.drop(['LTVCluster','m6_Revenue'],axis=1)
y = tx_class['LTVCluster']

#split training and test sets
from sklearn.model_selection import KFold, cross_val_score, train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=56)

import xgboost as xgb
ltv_xgb_model = xgb.XGBClassifier(max_depth=5, learning_rate=0.1,n_jobs=-1).fit(X_train, y_train)

print('Accuracy of XGB classifier on training set: {:.2f}'
       .format(ltv_xgb_model.score(X_train, y_train)))
print('Accuracy of XGB classifier on test set: {:.2f}'
       .format(ltv_xgb_model.score(X_test[X_train.columns], y_test)))

y_pred = ltv_xgb_model.predict(X_test)

from sklearn.model_selection import cross_val_score

cross_val=cross_val_score(ltv_xgb_model,X,y,cv=10,scoring='accuracy').mean()

cross_val

filename = 'SimpleXgboost'
pickle.dump(ltv_xgb_model, open(filename, 'wb'))

"""***Tuning XGB Model***"""

from sklearn.model_selection import RandomizedSearchCV

from scipy.stats import randint

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

params={
 "learning_rate"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,
 "max_depth"        : [ 3, 4, 5, 6, 8, 10, 12, 15],
 "min_child_weight" : [ 1, 3, 5, 7 ],
 "gamma"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],
 "colsample_bytree" : [ 0.3, 0.4, 0.5 , 0.7 ]
    
}

random_classifier = xgb.XGBClassifier()

random_search=RandomizedSearchCV(random_classifier,param_distributions=params,n_iter=5,n_jobs=-1,cv=5,verbose=3)

from sklearn import preprocessing
z = preprocessing.label_binarize(y, classes=[0, 1, 2, 3])

random_search.fit(X, y)
print(random_search.best_params_)

random_search.best_score_

tuned_xgb = xgb.XGBClassifier(min_child_weight=5,max_depth=4,learning_rate=0.05,gamma=0.0,colsample_bytree=0.4)

tuned_xgb.fit(X_train,y_train)

y_pred = tuned_xgb.predict(X_test)

from sklearn.metrics import confusion_matrix,accuracy_score
cm = confusion_matrix(y_test, y_pred)

accuracy_score=accuracy_score(y_test,y_pred)

accuracy_score

from sklearn.model_selection import cross_val_score

cross_val=cross_val_score(tuned_xgb,X,y,cv=10,scoring='accuracy').mean()

cross_val

filename = 'TunedXgboost'
pickle.dump(tuned_xgb, open(filename, 'wb'))

"""***Using Random Forest Classifier***"""

from sklearn.ensemble import RandomForestClassifier

est = RandomForestClassifier(n_jobs=-1)
rf_p_dist={'max_depth':[3,5,10,None],
              'n_estimators':[10,100,200,300,400,500],
              'max_features':randint(1,3),
               'criterion':['gini','entropy'],
               'bootstrap':[True,False],
               'min_samples_leaf':randint(1,4),
              }
def hypertuning_rscv(est, p_distr, nbr_iter,X,y):
    rdmsearch = RandomizedSearchCV(est, param_distributions=p_distr,
                                  n_jobs=-1, n_iter=nbr_iter, cv=9)
    #CV = Cross-Validation ( here using Stratified KFold CV)
    rdmsearch.fit(X,y)
    ht_params = rdmsearch.best_params_
    ht_score = rdmsearch.best_score_
    return ht_params, ht_score

rf_parameters, rf_ht_score = hypertuning_rscv(est, rf_p_dist, 40, X, y)

print(rf_parameters)

clasifier=RandomForestClassifier(bootstrap= False, criterion= 'gini', max_depth= None, max_features= 1, min_samples_leaf= 1, n_estimators= 200)

clasifier.fit(X_train,y_train)

y_pred = clasifier.predict(X_test)

from sklearn.metrics import confusion_matrix,accuracy_score
cm = confusion_matrix(y_test, y_pred)

accuracy_score=accuracy_score(y_test,y_pred)

accuracy_score

from sklearn.model_selection import cross_val_score

cross_val=cross_val_score(clasifier,X,y,cv=10,scoring='accuracy').mean()

cross_val

import pickle

filename = 'random_forest_retail.sav'
pickle.dump(clasifier, open(filename, 'wb'))

!ls

